---
title: ArrayList 和 LinkedList 的底层实现和适用场景是什么？HashMap 的底层实现（JDK1.7 vs 1.8）、ConcurrentHashMap 如何保证线程安全？HashMap 在多线程环境下可能出现什么问题？
published: 2025-05-01
description: ArrayList 和 LinkedList 的底层实现和适用场景是什么？HashMap 的底层实现（JDK1.7 vs 1.8）、ConcurrentHashMap 如何保证线程安全？HashMap 在多线程环境下可能出现什么问题？
tags: [Markdown, Blogging]
category: Java mianshi
licenseName: "Unlicensed"
author: Ankou
sourceLink: ''
draft: false
---
**ArrayList 和 LinkedList 的底层实现和适用场景是什么？HashMap 的底层实现（JDK1.7 vs 1.8）、ConcurrentHashMap 如何保证线程安全？HashMap 在多线程环境下可能出现什么问题？**

**1. ArrayList vs LinkedList：底层实现与适用场景**

- **ArrayList：**
  - **底层实现：** 基于**动态数组**。内部维护一个 `Object[] elementData`。当添加元素导致容量不足时，会自动进行扩容（通常增长为原容量的 1.5 倍，`int newCapacity = oldCapacity + (oldCapacity >> 1)`），创建一个更大的新数组并将旧数组元素复制过去。
  - 适用场景：
    - **频繁的随机访问 (`get(int index)`, `set(int index, E element)`):** 时间复杂度为 **O(1)**，因为通过索引可以直接计算出元素的内存偏移地址。
    - **尾部添加/删除元素 (`add(E e)`, `remove(int index)` 当 index 接近 size-1):** 时间复杂度接近 **O(1)**（不考虑扩容/缩容复制）。
  - 不适用场景：
    - **在列表中间频繁插入/删除元素 (`add(int index, E element)`, `remove(int index)` 当 index 接近 0):** 需要移动后续所有元素（平均移动 n/2 个元素），时间复杂度为 **O(n)**。性能较差。
    - **内存敏感场景：** 每次扩容都需要创建新数组并复制数据，可能产生内存浪费（尤其是设置过大初始容量时）和扩容时的瞬时开销。
- **LinkedList：**
  - **底层实现：** 基于**双向链表**。每个元素封装在一个 `Node` 节点中（包含元素本身 `item`、指向前驱节点的引用 `prev`、指向后继节点的引用 `next`）。维护头指针 `first` 和尾指针 `last`。
  - 适用场景：
    - **频繁在列表任意位置插入/删除元素 (`add(int index, E element)`, `remove(int index)`):** 时间复杂度为 **O(1)**（*定位节点*的时间是 O(n)，但*插入/删除操作本身*仅需改变相邻节点的引用，是 O(1)。通常我们说插入删除是 O(n) 是因为包含了定位的成本）。
    - **需要实现队列 (`Queue`) 或双端队列 (`Deque`) 功能：** `LinkedList` 天然实现了这些接口，`addFirst/addLast/removeFirst/removeLast` 等操作都是 O(1)。
  - 不适用场景：
    - **频繁的随机访问 (`get(int index)`, `set(int index, E element)`):** 需要从头或尾（根据索引位置选择）遍历链表定位节点，时间复杂度为 **O(n)**。性能很差。
    - **内存占用敏感：** 每个元素都需要额外的空间存储前驱和后继引用（比 `ArrayList` 的元素多占用约 12-16 字节/元素的指针开销）。

**总结选择：**

- 需要**快速随机访问**或主要在**尾部操作**，选择 **`ArrayList`**。
- 需要**频繁在列表中间插入删除**或需要**队列功能**，选择 **`LinkedList`**。

**2. HashMap 底层实现 (JDK1.7 vs 1.8)**

- **共同基础：**
  - 基于 **数组 + 链表** 的 "桶" (bucket) 结构。
  - 核心思想：通过 `key` 的 `hashCode()` 计算出一个哈希值 (`hash`)，再通过 `(n - 1) & hash` （或 `hash % n`，等价但要求 n 是 2 的幂）计算出该 `key` 应放入的**桶索引 (bucket index)**。`n` 是数组长度。
  - 解决哈希冲突：当多个 `key` 的哈希值映射到同一个桶时（哈希冲突），这些 `key-value` 对会以链表形式存储在该桶中（链地址法）。
- **JDK 1.7:**
  - **结构:** 数组 `Entry[] table` + 单向链表 (`Entry` 节点包含 `key`, `value`, `hash`, `next` 指针)。
  - **插入方式：头插法。** 新节点插入到链表的头部（`newEntry.next = table[i]; table[i] = newEntry;`）。**这是导致多线程死循环的关键原因。**
  - 扩容 (Rehashing):
    1. 当 `size > threshold` (容量 `capacity` * 负载因子 `loadFactor`) 时触发扩容。
    2. 创建新数组（大小为原数组的 2 倍）。
    3. 遍历旧数组每个桶和链表。
    4. 对每个 `Entry`，重新计算其在新数组中的索引 `(newCapacity - 1) & hash`。
    5. **采用头插法**将节点转移到新数组对应的桶链表中。**此过程在多线程并发扩容时容易造成环形链表，导致后续 `get()` 操作死循环或数据丢失。**
  - **哈希函数：** `hash = hash(key.hashCode())` (进行了一些扰动计算，但相对简单)。
- **JDK 1.8 (及以后):**
  - **结构：** 数组 `Node[] table` + 单向链表 / 红黑树 (`TreeNode`)。当链表长度超过阈值 (`TREEIFY_THRESHOLD = 8`) 且数组长度达到一定大小 (`MIN_TREEIFY_CAPACITY = 64`)，链表会转换为**红黑树**。当树节点数小于阈值 (`UNTREEIFY_THRESHOLD = 6`)，树会退化为链表。
  - **插入方式：尾插法。** 新节点插入到链表的尾部（需要遍历到尾部再插入）。**消除了并发扩容时形成环形链表的可能性。**
  - 扩容 (Resizing):
    1. 触发条件相同 (`size > threshold`)。
    2. 创建新数组（大小为原数组的 2 倍）。
    3. 更智能的 rehash:由于数组长度n总是 2 的幂，扩容后newIndex = oldIndex或newIndex = oldIndex + oldCapacity。只需检查(e.hash & oldCapacity) == 0：
       - 如果为 0，节点在新数组中的索引 = 原索引 (`oldIndex`)。
       - 如果不为 0，节点在新数组中的索引 = 原索引 + 原容量 (`oldIndex + oldCapacity`)。
    4. 按上述规则将原桶中的节点 (链表或树) **拆分**到新数组的两个桶中（`low` 头尾指针指向索引不变的部分，`high` 头尾指针指向索引增加的部分）。
    5. 使用**尾插法**将 `low` 和 `high` 链表/树放到新数组对应的桶中。
    6. 此方法避免了对每个节点重新计算 `hash`（只需用 `hash & oldCapacity` 判断高位），效率更高。
  - **哈希函数：** 更复杂的扰动函数 (`(h = key.hashCode()) ^ (h >>> 16)`)。将 `hashCode` 的高 16 位异或到低 16 位，增加了低位的随机性，减少哈希冲突。
  - **优化查找：** 引入红黑树，将链表查找 (`O(n)`) 优化为树查找 (`O(log n)`) 在哈希冲突严重时显著提升性能。

**3. ConcurrentHashMap 如何保证线程安全 (JDK1.7 vs 1.8)**

- **共同目标：** 提供线程安全的、高并发的 `HashMap` 实现。
- **JDK 1.7:**
  - 结构：分段锁 (Segment Locking / Striping)
    - 整个 `ConcurrentHashMap` 由一组 `Segment` 对象组成（默认 16 个）。每个 `Segment` 本质上是一个小的、独立的 `ReentrantLock` 和 `HashMap` (包含一个 `HashEntry[]` 数组)。
    - `Segment` 继承自 `ReentrantLock`，充当该段的锁。
  - 保证线程安全机制：
    - **锁分段：** 不同的 `Segment` 上的操作可以**并发**进行。一个线程只需要锁住操作涉及的那个特定 `Segment`，而其他 `Segment` 仍然可以被其他线程访问。锁的粒度从整个 `Map` 细化到 `Segment`。
    - **段内锁：** 对某个 `Segment` 内部的 `HashEntry` 数组进行写操作（`put`, `remove`, `replace`）时，需要先获取该 `Segment` 的锁。读操作 (`get`) 通常不需要加锁（使用 `volatile` 保证可见性）。
    - **不变性 (Immutable Objects):** `HashEntry` 的 `key`, `hash`, `next` 指针被声明为 `final`（`value` 为 `volatile`）。这保证了在遍历链表时，即使链表结构被修改（只能在头部添加新节点），不会影响正在进行的读操作（读操作看到的是旧的链表结构快照），从而避免了读操作加锁。新节点采用**头插法**。
  - **缺点：** 锁的粒度虽然比 `Hashtable` 细，但仍然是 `Segment` 级别。并发度受限于 `Segment` 的数量（默认 16）。对于非常大的 `Map` 或极高并发，锁竞争可能成为瓶颈。扩容是段内独立扩容。
- **JDK 1.8 (及以后):**
  - **结构：** 放弃了分段锁，采用与 `HashMap 1.8` 相似的 **`Node[] table` 数组 + 链表/红黑树**。
  - 保证线程安全机制：
    - CAS + `synchronized` 细粒度锁：
      - `put`/`compute` 等写操作的核心：
        1. 根据 `key` 的 `hash` 定位到具体的桶（数组元素）。
        2. 如果桶为空 (`null`)，尝试使用 **CAS (Compare-And-Swap)** 操作将新节点放入桶中。成功则插入完成。失败说明发生竞争，进入步骤 3。
        3. 如果桶不为空，或者 CAS 失败，则**锁住该桶的头节点**（使用 `synchronized` 关键字）。锁的粒度细化到了**单个桶**级别，并发度大大提高（理论上和桶的数量一致）。
        4. 在锁的保护下，遍历链表或树：
           - 如果是链表：查找是否存在相同 `key` 的节点，存在则更新 `value`，不存在则添加到链表**尾部**（尾插法）。
           - 如果是红黑树：调用树的 `putTreeVal` 方法。
        5. 检查是否需要树化或扩容。
      - **`get` 读操作：** 通常**不需要加锁**！因为 `Node` 的 `value` 和 `next` 指针都用 `volatile` 修饰，保证了线程间的可见性。CAS 和 `volatile` 的配合确保了读取时能看到最新写入的值。
    - 扩容机制 (Transfer)：
      - 仍然是多线程并发扩容。当触发扩容时，会有一个线程负责创建新数组，并将原数组划分为多个“步幅”(stride)区间。
      - 当其他线程进行 `put` 操作时，如果发现正在扩容（通过一个标识状态 `sizeCtl` 判断），会尝试帮助迁移一部分桶的数据（称为“协助迁移”），分摊迁移压力。
      - 迁移规则与 `HashMap 1.8` 相同（`(e.hash & oldCapacity) == 0` 拆分到两个桶）。迁移过程中锁住的是原数组的单个桶（`synchronized`）。
    - **`size()`/`mappingCount()`：** 使用一个 `volatile` 的基本计数器 `baseCount` 和一个 `CounterCell[]` 数组（类似 `LongAdder` 的思想）。在无竞争时修改 `baseCount`；发生竞争时，线程尝试修改随机选中的 `CounterCell` 的值。最后通过累加 `baseCount` 和所有 `CounterCell` 的值得到总数。避免了全局锁竞争，提高了并发度。
  - **优点：** 锁的粒度更细（桶级别），并发度远高于 JDK7 的分段锁。数据结构更简洁（与 `HashMap` 统一）。利用 CAS 和 `volatile` 减少了锁的使用，读操作完全无锁，写操作只有在冲突时才锁桶。

**4. HashMap 在多线程环境下可能出现什么问题？**

在多线程环境下同时修改 `HashMap`（特别是进行 `put` 操作），即使没有使用 `ConcurrentHashMap`，也会导致各种严重问题：

1. **数据不一致/丢失：** 这是最常见的问题。两个线程同时 `put` 不同的键值对到同一个桶（链表）时，由于操作不是原子的，一个线程的写入可能被另一个线程的写入覆盖，导致其中一个键值对丢失。

2. 死循环 (JDK 1.7 特有)：这是最严重、最隐蔽的问题，发生在多个线程同时触发HashMap扩容 (`resize()`) 时。

   - 原因： JDK 1.7 的HashMap在扩容迁移链表节点时采用头插法。考虑两个线程 A 和 B 同时扩容：

     1. 假设原链表：`Entry1 -> Entry2` (桶索引为 i)。
     2. 线程 A 开始迁移，刚执行完 `Entry1.next = newTable[i];`（此时 `newTable[i]` 是 `Entry1`，`Entry1.next = null`），然后被挂起。
     3. 线程 B 开始扩容并完成迁移。它创建的新链表是：`Entry2 -> Entry1`（头插法导致逆序）。线程 B 执行完毕。
     4. 线程 A 恢复执行，此时它看到的 `table[i]` 还是 `Entry1`（因为线程 B 修改的是新数组，线程 A 有自己的新数组引用）。线程 A 接着处理 `Entry2`：`Entry2.next = newTable[i];` 即 `Entry2.next = Entry1`。然后将 `newTable[i]` 指向 `Entry2`。
     5. **结果：** 在线程 A 的新数组中，链表变成了 `Entry2 -> Entry1`。但注意 `Entry1.next` 已经被线程 A 在第一步设置为 `null` 了，而线程 B 迁移时让 `Entry1.next = Entry2`（在 B 的新数组中）。现在链表是 `Entry2 -> Entry1`，而 `Entry1.next = null`。
     6. 表面看没问题，但关键在于线程 B 已经完成并设置了 `HashMap` 的 `table` 引用指向它的新数组（包含 `Entry2 -> Entry1`）。线程 A 最后也会设置 `table` 指向它的新数组（也包含 `Entry2 -> Entry1`）。这本身不会死循环。

     - **触发死循环的条件：** 如果在上述步骤 3 中，线程 B 迁移时，原链表是 `Entry1 -> Entry2 -> Entry3`（三个节点）。线程 B 迁移后新链表可能是 `Entry3 -> Entry2 -> Entry1`（头插法逆序）。当线程 A 恢复后，它看到的原链表节点顺序还是 `Entry1 -> Entry2 -> Entry3`（线程 B 修改的是新数组，不影响线程 A 看到的旧数组）。线程 A 迁移 `Entry1`（设置 `Entry1.next = null` 并放入新桶），然后迁移 `Entry2`（设置 `Entry2.next = Entry1`），然后迁移 `Entry3`（设置 `Entry3.next = Entry2`）。最终在线程 A 的新桶中，链表是 `Entry3 -> Entry2 -> Entry1`，但 `Entry1.next = null`。**问题在于 `Entry2.next = Entry1` 和 `Entry1.next = null`，这没有环。**
     - **真正的死循环触发点在于并发头插法导致链表节点的 `next` 指针错乱形成环。** 更典型的场景是：两个线程都在迁移同一个桶的链表，并且都采用头插法向新桶插入节点。由于线程调度和头插法的特性，可能导致 `A.next = B` 和 `B.next = A` 的局面，形成一个环形链表。后续对该桶的任何遍历操作（`get`, `put` 冲突, `size` 等）都将陷入无限循环，CPU 占用率飙升。

   - **JDK 1.8 解决：** 采用尾插法 + 扩容时直接根据高位比特位拆分链表到新桶的两个位置，不再需要反转链表顺序，彻底消除了扩容时形成环形链表的可能性。

3. **`get()` 到 `null` 或旧值：** 由于可见性问题（没有 `volatile` 保证），一个线程 `put` 了新值，另一个线程可能看不到最新的 `value`（读取到 `null` 或旧值）。或者由于数据正在被迁移，读取到不完整的状态。

4. **`size()` 不准确：** `HashMap` 的 `size` 不是原子的。多个线程同时 `put` 成功会导致 `size` 的实际增加值少于线程数（因为 `size++` 不是原子操作）。或者扩容过程中读取 `size` 可能得到中间状态值。

**总结：** 由于 `HashMap` 在设计上就不是线程安全的，任何在多线程环境下的并发修改（即使只是读和写并发）都可能导致数据损坏、状态不一致、甚至程序崩溃（死循环）。**绝对禁止在多线程环境下直接使用 `HashMap` 进行并发更新。** 务必使用线程安全的 `ConcurrentHashMap` 或通过外部同步机制（如 `Collections.synchronizedMap(new HashMap<>())` 或显式锁）来保护对 `HashMap` 的访问。`ConcurrentHashMap` 在 JDK 1.8 的实现提供了更高的并发性能和安全性。